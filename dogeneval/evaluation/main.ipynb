{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 27/100 [01:24<03:47,  3.12s/it]\n"
     ]
    }
   ],
   "source": [
    "import semantic_metrics\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import importlib\n",
    "importlib.reload(semantic_metrics)\n",
    "from semantic_metrics import calculate_avg_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1873\n"
     ]
    }
   ],
   "source": [
    "# 加载hdx生成的QA数据\n",
    "from dogeneval.utils.mongodb import load_results\n",
    "qa_results = load_results(collection_name=\"QA_GEN-11022040\")\n",
    "\n",
    "qa_instructs = [f\"{qa['Question']}\\n{qa['Answer']}\" for qa in qa_results]\n",
    "print(len(qa_instructs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15687"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载mmlu的指令集\n",
    "mmlu_dev = \"/home/junetheriver/codes/qa_generation/huawei/data/mmlu/dev\"\n",
    "mmlu_test = \"/home/junetheriver/codes/qa_generation/huawei/data/mmlu/test\"\n",
    "mmlu_val = \"/home/junetheriver/codes/qa_generation/huawei/data/mmlu/val\"\n",
    "\n",
    "# 读取目录下所有csv文件，将里面的题目保存在一个list中\n",
    "mmlu_instructs = []\n",
    "for dir in [mmlu_dev, mmlu_test, mmlu_val]:\n",
    "    for file in os.listdir(dir):\n",
    "        df = pd.read_csv(os.path.join(dir, file))\n",
    "        # 将每行的所有列的内容拼接，df中的列没有名字\n",
    "        df['merged'] = df.apply(lambda row: ''.join([str(col) for col in row]), axis=1)\n",
    "        mmlu_instructs.extend(df['merged'].tolist())    \n",
    "\n",
    "len(mmlu_instructs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_instructs(instructs, sample_size, seed=0):\n",
    "    random.seed(seed)\n",
    "    return random.sample(instructs, sample_size)\n",
    "\n",
    "sampled_qa_instructs = sample_instructs(qa_instructs, 100)\n",
    "sampled_mmlu_instructs = sample_instructs(mmlu_instructs, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-11-11 16:16:45.098\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msemantic_metrics\u001b[0m:\u001b[36mcalculate_avg_similarity\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1mCalculating average similarity for 100 instructions...\u001b[0m\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|██████████| 100/100 [00:43<00:00,  2.31it/s]\n",
      "\u001b[32m2024-11-11 16:17:28.452\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msemantic_metrics\u001b[0m:\u001b[36mcalculate_avg_similarity\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mEmbeddings generated for 100 instructions.\u001b[0m\n",
      "\u001b[32m2024-11-11 16:17:28.464\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msemantic_metrics\u001b[0m:\u001b[36mcalculate_avg_similarity\u001b[0m:\u001b[36m25\u001b[0m - \u001b[1mCosine similarity matrix calculated for 100 instructions.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5928722300696991\n"
     ]
    }
   ],
   "source": [
    "\n",
    "avg_similarity = calculate_avg_similarity(sampled_qa_instructs)\n",
    "print(avg_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-11-11 16:17:38.316\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msemantic_metrics\u001b[0m:\u001b[36mcalculate_avg_similarity\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1mCalculating average similarity for 100 instructions...\u001b[0m\n",
      "100%|██████████| 100/100 [00:18<00:00,  5.47it/s]\n",
      "\u001b[32m2024-11-11 16:17:56.598\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msemantic_metrics\u001b[0m:\u001b[36mcalculate_avg_similarity\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mEmbeddings generated for 100 instructions.\u001b[0m\n",
      "\u001b[32m2024-11-11 16:17:56.601\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msemantic_metrics\u001b[0m:\u001b[36mcalculate_avg_similarity\u001b[0m:\u001b[36m25\u001b[0m - \u001b[1mCosine similarity matrix calculated for 100 instructions.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4639597389547528\n"
     ]
    }
   ],
   "source": [
    "sampled_mmlu_avg_similarity = calculate_avg_similarity(sampled_mmlu_instructs)\n",
    "print(sampled_mmlu_avg_similarity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_sampling(instructs, sample_size, num_folds=10):\n",
    "    avg_similarities = []\n",
    "    for i in range(num_folds):\n",
    "        sampled_instructs = sample_instructs(instructs, sample_size, seed=i)\n",
    "        avg_similarity = calculate_avg_similarity(sampled_instructs)\n",
    "        avg_similarities.append(avg_similarity)\n",
    "    return np.mean(avg_similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.586265983395978\n",
      "0.465850584289243\n"
     ]
    }
   ],
   "source": [
    "qa_avg_similarity = average_sampling(qa_instructs, 100)\n",
    "print(qa_avg_similarity)\n",
    "mmlu_avg_similarity = average_sampling(mmlu_instructs, 100)\n",
    "print(mmlu_avg_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opencompass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
